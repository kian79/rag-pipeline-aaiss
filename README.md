# Retrieval-Augmented Generation (RAG) with LangChain & MistralAI ðŸš€
This repository contains the materials, code, and hands-on examples from my workshop on Retrieval-Augmented Generation (RAG) at the Amirkabir Artificial Intelligence Student Summit (AAISS 2025).

- Workshop Topic: RAG in Action: Enhancing Large Language Models (LLMs) with external knowledge using RAG, Pinecone, and LangChain.
- Focus Areas: Vector databases, prompt engineering, embedding models, and multilingual retrieval.
# ðŸŽ¯ Workshop Overview
In this session, we explored:
- What are Transformers? What is attention mechanism?
- Basics of LLMs. How do transformers improved NLP and are being used in LLMs.
- What is RAG? How it differs from fine-tuning.
- How Vector Databases Work for storing and retrieving document embeddings?
- Building a RAG Pipeline with MistralAI, Pinecone, and Hugging Face Embeddings.
- Hands-on Demo: Querying PDF documents with a real-time retrieval-based LLM system.
- Prompt Engineering: In-context learning, chain-of-thought, structured outputs.

These materials provide an end-to-end practical implementation of RAG using LangChain and MistralAI, covering everything from PDF ingestion to Pinecone-based retrieval and query generation.

# Workshop Takeaways
This workshop demonstrated:
- How RAG enhances LLMs with real-time document retrieval.
- The role of vector databases like Pinecone in scalable search.
- How prompt engineering improves model accuracy.
- Hands-on coding examples for PDF ingestion, embedding, retrieval, and response generation.

If you attended the session or have questions about RAG, feel free to reach out!


![First Slide](https://github.com/kian79/rag-pipeline-aaiss/blob/main/Screenshot%20from%202025-02-21%2017-16-41.png)
